{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update - Download New Daily OISSTv2 Data\n",
    "\n",
    "This is foundational step towards continuous integration. \n",
    "\n",
    "In this notebook we use Beautifulsoup to ping the url for the most recently available OISST data, and join it with the rest of the data for the current year. This works in a way to replace preliminary data as finalized data becomes available while keeping our data current. Preliminary data is re-downloading and replaced with finalized data, and the most recent OISST data (including prelim files) is exported to be used for the remaining workflows. This step is the primer for any subsequent `Update`  workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Set the workspace - local/ docker\n",
    "workspace = \"local\"\n",
    "\n",
    "# Root paths\n",
    "root_locations = {\"local\"  : \"/Users/akemberling/Box/\",\n",
    "                  \"docker\" : \"/home/jovyan/\"}\n",
    "\n",
    "\n",
    "# Set root\n",
    "box_root = root_locations[workspace]\n",
    "print(f\"Working via {workspace} directory at: {box_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Destinations for Downloads and Updated Files\n",
    "\n",
    "In this section we specify what year/months we are interested in, and where we want the daily netcdf files to be downloaded to. I also set up a dictionary for quick-access to other resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder for daily caches:      RES_Data/OISST/oisst_mainstays/update_caches/12/\n",
      "Access folder for yearly aggregates: RES_Data/OISST/oisst_mainstays/annual_observations/\n"
     ]
    }
   ],
   "source": [
    "# Update year to search for among links\n",
    "update_yr = 2021\n",
    "update_month = 12\n",
    "\n",
    "# Out Destination - Cache gobals\n",
    "_cache_root = f\"{box_root}RES_Data/OISST/oisst_mainstays/\"\n",
    "\n",
    "# Cache Subdirectory Locations\n",
    "cache_locs = {\n",
    "  \"cache_root\"        : f\"{_cache_root}\",\n",
    "  \"annual_obs\"        : f\"{_cache_root}annual_observations/\",\n",
    "  \"daily_file_caches\" : f\"{_cache_root}update_caches/{update_month}/\",\n",
    "}\n",
    "\n",
    "# Set the output location for where things should save to:\n",
    "cache_loc = cache_locs[\"daily_file_caches\"] # Where daily caches should go\n",
    "annual_loc = cache_locs[\"annual_obs\"]       # Where the complete year files live\n",
    "\n",
    "# Print paths to validate\n",
    "print(\"Output folder for daily caches:      \" + cache_loc)\n",
    "print(\"Access folder for yearly aggregates: \" + annual_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Root url for Web Scraping\n",
    "\n",
    "This part is where we identify the web pages where the files and their directories can be accessed. `update_yr` and `update_month` are use to navigate to the right folders for updating the correct files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root url where the yearly files are stored\n",
    "\n",
    "# This url does not work as it is the return for a data query. When using bs4 it scrapes the site prior to the search\n",
    "# https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html\n",
    "\n",
    "\n",
    "# This URL is from Eric Bridgers repo oisst-clim-daily\n",
    "# Use the url from Eric's Repo, thank you Eric\n",
    "\n",
    "# This URL will grab the desired update month - need to add functionality for the transition to new months\n",
    "fetch_url = f\"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/{update_yr}{update_month}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the http directory listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the http directory listing.\n",
    "req = requests.get(fetch_url)\n",
    "\n",
    "# Print error message if link does not work\n",
    "if req.status_code != requests.codes.ok:\n",
    "    #logger.info(\"{} {} {}\".format(req.status_code, req.reason, req.url))\n",
    "    print(f\"Request Error, Reason: {req.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse URL contents with BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Parse the url with BS and its html parser.\n",
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once parsed, the next step is to look for the correct html elements to access the files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found File: oisst-avhrr-v02r01.20201201.nc\n",
      "Found File: oisst-avhrr-v02r01.20201202.nc\n",
      "Found File: oisst-avhrr-v02r01.20201203.nc\n",
      "Found File: oisst-avhrr-v02r01.20201204.nc\n",
      "Found File: oisst-avhrr-v02r01.20201205.nc\n",
      "Found File: oisst-avhrr-v02r01.20201206.nc\n",
      "Found File: oisst-avhrr-v02r01.20201207.nc\n",
      "Found File: oisst-avhrr-v02r01.20201208.nc\n",
      "Found File: oisst-avhrr-v02r01.20201209.nc\n",
      "Found File: oisst-avhrr-v02r01.20201210.nc\n",
      "Found File: oisst-avhrr-v02r01.20201211.nc\n",
      "Found File: oisst-avhrr-v02r01.20201212.nc\n",
      "Found File: oisst-avhrr-v02r01.20201213.nc\n",
      "Found File: oisst-avhrr-v02r01.20201214.nc\n",
      "Found File: oisst-avhrr-v02r01.20201215.nc\n",
      "Found File: oisst-avhrr-v02r01.20201215_preliminary.nc\n",
      "Found File: oisst-avhrr-v02r01.20201216.nc\n",
      "Found File: oisst-avhrr-v02r01.20201216_preliminary.nc\n",
      "Found File: oisst-avhrr-v02r01.20201217.nc\n",
      "Found File: oisst-avhrr-v02r01.20201217_preliminary.nc\n",
      "Found File: oisst-avhrr-v02r01.20201218.nc\n",
      "Found File: oisst-avhrr-v02r01.20201219.nc\n",
      "Found File: oisst-avhrr-v02r01.20201220.nc\n",
      "Found File: oisst-avhrr-v02r01.20201221.nc\n",
      "Found File: oisst-avhrr-v02r01.20201222.nc\n",
      "Found File: oisst-avhrr-v02r01.20201223.nc\n",
      "Found File: oisst-avhrr-v02r01.20201224.nc\n",
      "Found File: oisst-avhrr-v02r01.20201225.nc\n",
      "Found File: oisst-avhrr-v02r01.20201226.nc\n",
      "Found File: oisst-avhrr-v02r01.20201227.nc\n",
      "Found File: oisst-avhrr-v02r01.20201228.nc\n",
      "Found File: oisst-avhrr-v02r01.20201229.nc\n",
      "Found File: oisst-avhrr-v02r01.20201230.nc\n",
      "Found File: oisst-avhrr-v02r01.20201231.nc\n"
     ]
    }
   ],
   "source": [
    "# Find all href anchors in the html text\n",
    "anchors = soup.find_all(\"a\")\n",
    "for link in anchors:\n",
    "    #print(link)\n",
    "    if link.get(\"href\").endswith(\"nc\"):\n",
    "        href = link.get(\"href\")\n",
    "        print(f\"Found File: {href}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Netcdf's to Cache Folder\n",
    "\n",
    "Once we have verified what daily files are available the next step is to download each of them to the cache in their current daily file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201201.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201202.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201203.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201204.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201205.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201206.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201207.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201208.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201209.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201210.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201211.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201212.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201213.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201214.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201215.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201215_preliminary.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201216.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201216_preliminary.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201217.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201217_preliminary.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201218.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201219.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201220.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201221.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201222.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201223.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201224.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201225.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201226.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201227.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201228.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201229.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201230.nc\n",
      "Cached Daily NETCDF File: oisst-avhrr-v02r01.20201231.nc\n"
     ]
    }
   ],
   "source": [
    "# list to store download paths\n",
    "new_downloads = []\n",
    "\n",
    "# Find all the links in fetch_url which end with \".nc\"\n",
    "for link in anchors:\n",
    "    \n",
    "    # Find links that match update year\n",
    "    if link.get('href').endswith(f'.nc'):\n",
    "        \n",
    "        # Get the link(s) that match\n",
    "        href = link.get('href')\n",
    "        #print(f\"Download link match: {href}\")\n",
    "        \n",
    "        # Use requests to download\n",
    "        req_link = fetch_url + href\n",
    "        req = requests.get(fetch_url + href)\n",
    "        if req.raise_for_status():\n",
    "            exit()\n",
    "        \n",
    "        # Open link\n",
    "        dl_path = f\"{cache_loc}{href}\"\n",
    "        file = open(dl_path, 'wb')\n",
    "        chunk_size = 17000000\n",
    "        \n",
    "        # Add to log\n",
    "        new_downloads.append(dl_path)\n",
    "        \n",
    "        # Process in chunks to save daily files\n",
    "        for chunk in req.iter_content(chunk_size):\n",
    "            file.write(chunk)\n",
    "        file.close()\n",
    "        print(f\"Cached Daily NETCDF File: {href}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the files we just accessed and cached:\n",
    "\n",
    "Now that the daily files have been accessed and saved individually, the next step is to stack them all up and align them with the annual netcdf file. Things that need to nbe done before that happens are:\n",
    " * The removal of duplicated dates\n",
    " * The overwriting of preliminary data if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Find the dates with preliminary data, flag them\n",
    "# #\n",
    "\n",
    "# prelim_dates = []\n",
    "# for link in new_downloads:\n",
    "#     if link.endswith(\"_preliminary.nc\"): \n",
    "#         start_idx = link.find(f\"{update_yr}{update_month}\")\n",
    "#         end_idx = start_idx + 8\n",
    "#         step = int(1)\n",
    "#         date_id = link[start_idx: end_idx: step]\n",
    "#         prelim_dates.append(date_id)\n",
    "        \n",
    "# print(\"Preliminary data found for: \")\n",
    "# prelim_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preliminary data found for: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['20201215',\n",
       " '20201216',\n",
       " '20201217',\n",
       " '20201225',\n",
       " '20201226',\n",
       " '20201227',\n",
       " '20201228',\n",
       " '20201229',\n",
       " '20201230',\n",
       " '20201231']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Find the dates with preliminary data, flag them\n",
    "#\n",
    "\n",
    "# CHANGE:\n",
    "# should look for preliminary files in the cache folder and not the download list,\n",
    "# this way no files slip through the cracks\n",
    "# os.listdir(cache_loc)\n",
    "\n",
    "prelim_dates = []\n",
    "for cache_file in os.listdir(cache_loc):\n",
    "    if cache_file.endswith(\"_preliminary.nc\"): \n",
    "        start_idx = cache_file.find(f\"{update_yr}{update_month}\")\n",
    "        end_idx = start_idx + 8\n",
    "        step = int(1)\n",
    "        date_id = cache_file[start_idx: end_idx: step]\n",
    "        prelim_dates.append(date_id)\n",
    "        \n",
    "print(\"Preliminary data found for: \")\n",
    "prelim_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the preliminary data files have been flagged, the next step is to check if those dates now have files without _preliminary on the end, indicating that the data has been finalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prelim and Finalized Data Found for:\n",
      "['20201225', '20201226', '20201227', '20201228', '20201229', '20201230', '20201231']\n"
     ]
    }
   ],
   "source": [
    "# 2. Check if those dates only have prelim data, or if there is also finalized data:\n",
    "\n",
    "# list of duplicated dates, dates where we can ignore the preliminary data\n",
    "finalized_dates = []\n",
    "\n",
    "# check all download links\n",
    "for link in os.listdir(cache_loc):\n",
    "    \n",
    "    # check each preliminary data date\n",
    "    for prelim_date in prelim_dates:\n",
    "        \n",
    "        # If the date for the link matches dates with preliminary data for a link, flag the date\n",
    "        if prelim_date in link:\n",
    "            start_idx = link.find(f\"{update_yr}{update_month}\")\n",
    "            end_idx = start_idx + 8\n",
    "            step = int(1)\n",
    "            date_id = link[start_idx: end_idx: step]\n",
    "            \n",
    "            # and add those to the list of dates where there is prelim and final data\n",
    "            finalized_dates.append(date_id)\n",
    "            \n",
    "\n",
    "\n",
    "# Program to check for repeated list contents\n",
    "# This code is contributed  \n",
    "# by Sandeep_anand \n",
    "def Repeat(x): \n",
    "    _size = len(x) \n",
    "    repeated = [] \n",
    "    for i in range(_size): \n",
    "        k = i + 1\n",
    "        for j in range(k, _size): \n",
    "            if x[i] == x[j] and x[i] not in repeated: \n",
    "                repeated.append(x[i]) \n",
    "    return repeated \n",
    "  \n",
    "    \n",
    "    \n",
    "# Report the dates with both preliminary and finalized data  \n",
    "print(\"Prelim and Finalized Data Found for:\")\n",
    "print (Repeat(finalized_dates))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove preliminaries that have final data\n",
    "\n",
    "If any preliminary files exist the next step is to remove them so we don't get duplicate dates when we combine the individual netcdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201225_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201226_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201227_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201228_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201229_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201230_preliminary.nc\n",
      "File Removed for Finalized Data: RES_Data/OISST/oisst_mainstays/update_caches/12/oisst-avhrr-v02r01.20201231_preliminary.nc\n"
     ]
    }
   ],
   "source": [
    "# Pull the dates for situations where there is both a preliminary and final file.\n",
    "remove_prelim = Repeat(finalized_dates)\n",
    "\n",
    "# Build out the preliminary names that these would be, drop them from cache.\n",
    "for repeated_date in remove_prelim:\n",
    "    \n",
    "    # Build full file name\n",
    "    file_name = f\"{cache_loc}oisst-avhrr-v02r01.{repeated_date}_preliminary.nc\"\n",
    "    \n",
    "    # Print the ones we removed    \n",
    "    print(f\"File Removed for Finalized Data: {file_name}\")\n",
    "    \n",
    "    # Remove them from the folder. Don't need them anymore.\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Annual File\n",
    "\n",
    "Now that each daily file has been checked for updates, with duplicates removed we can start gluing everything together again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended File: oisst-avhrr-v02r01.20201201.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201202.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201203.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201204.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201205.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201206.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201207.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201208.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201209.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201210.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201211.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201212.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201213.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201214.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201215.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201216.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201217.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201218.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201219.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201220.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201221.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201222.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201223.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201224.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201225.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201226.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201227.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201228.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201229.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201230.nc\n",
      "Appended File: oisst-avhrr-v02r01.20201231.nc\n"
     ]
    }
   ],
   "source": [
    "# First generate new list of filenames since removing the duplicates etc.\n",
    "daily_files = []\n",
    "for file in os.listdir(f\"{cache_loc}\"):\n",
    "    if file.endswith(\".nc\"):\n",
    "        daily_files.append(f\"{cache_loc}{file}\")\n",
    "        print(f\"Appended File: {file}\")\n",
    "\n",
    "        \n",
    "# Use open_mfdataset to access the new downloads\n",
    "oisst_update = xr.open_mfdataset(daily_files, combine = \"by_coords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;sst&#x27; (time: 31, zlev: 1, lat: 720, lon: 1440)&gt;\n",
       "dask.array&lt;getitem, shape=(31, 1, 720, 1440), dtype=float32, chunksize=(1, 1, 720, 1440), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * time     (time) datetime64[ns] 2020-12-01T12:00:00 ... 2020-12-31T12:00:00\n",
       "Dimensions without coordinates: zlev\n",
       "Attributes:\n",
       "    long_name:  Daily sea surface temperature\n",
       "    units:      Celsius\n",
       "    valid_min:  -300\n",
       "    valid_max:  4500</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'sst' (time: 31, zlev: 1, lat: 720, lon: 1440)>\n",
       "dask.array<getitem, shape=(31, 1, 720, 1440), dtype=float32, chunksize=(1, 1, 720, 1440), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * time     (time) datetime64[ns] 2020-12-01T12:00:00 ... 2020-12-31T12:00:00\n",
       "Dimensions without coordinates: zlev\n",
       "Attributes:\n",
       "    long_name:  Daily sea surface temperature\n",
       "    units:      Celsius\n",
       "    valid_min:  -300\n",
       "    valid_max:  4500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check time index\n",
    "#oisst_update.get_index(\"time\")\n",
    "\n",
    "# Get all dates where the time indexes are not (~) duplicated\n",
    "oisst_noreps = oisst_update.sel(time = ~oisst_update.get_index(\"time\").duplicated())\n",
    "\n",
    "# drop additional coordinates and variables not in the annual file\n",
    "update_prepped = oisst_noreps#.drop(\"zlev\")\n",
    "update_prepped = update_prepped.drop_vars([\"anom\", \"err\", \"ice\"])\n",
    "update_prepped\n",
    "\n",
    "# remove attributes, going to add back later\n",
    "update_prepped.attrs = {}\n",
    "\n",
    "# dropping zlev\n",
    "update_prepped.sst.drop(\"zlev\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Steps:\n",
    "\n",
    "## Load and Append to Yearly File\n",
    "\n",
    "On the new year this will change: But currently we have an annual file of the same structure as all the others. We want to append on all the new days, avoiding any overlap/repetition.\n",
    "\n",
    "Once that is done we just need to format everything and it can be saved out as the annual netcdf with daily increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (lat: 720, lon: 1440, time: 366)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2020-01-01 2020-01-02 ... 2020-12-31\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "Data variables:\n",
       "    sst      (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:    CF-1.5\n",
       "    title:          NOAA/NCEI 1/4 Degree Daily Optimum Interpolation Sea Surf...\n",
       "    institution:    NOAA/National Centers for Environmental Information\n",
       "    source:         NOAA/NCEI https://www.ncei.noaa.gov/data/sea-surface-temp...\n",
       "    References:     https://www.psl.noaa.gov/data/gridded/data.noaa.oisst.v2....\n",
       "    dataset_title:  NOAA Daily Optimum Interpolation Sea Surface Temperature\n",
       "    version:        Version 2.1\n",
       "    comment:        Reynolds, et al.(2007) Daily High-Resolution-Blended Anal...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 720, lon: 1440, time: 366)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2020-01-01 2020-01-02 ... 2020-12-31\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "Data variables:\n",
       "    sst      (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:    CF-1.5\n",
       "    title:          NOAA/NCEI 1/4 Degree Daily Optimum Interpolation Sea Surf...\n",
       "    institution:    NOAA/National Centers for Environmental Information\n",
       "    source:         NOAA/NCEI https://www.ncei.noaa.gov/data/sea-surface-temp...\n",
       "    References:     https://www.psl.noaa.gov/data/gridded/data.noaa.oisst.v2....\n",
       "    dataset_title:  NOAA Daily Optimum Interpolation Sea Surface Temperature\n",
       "    version:        Version 2.1\n",
       "    comment:        Reynolds, et al.(2007) Daily High-Resolution-Blended Anal..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the yearly file we're appending to\n",
    "oisst = xr.open_dataset(f\"{annual_loc}sst.day.mean.{update_yr}.v2.nc\")\n",
    "oisst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time.month'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-59575c625e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Subset time so there isn't overlap on update month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moisst_prepped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moisst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_update_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moisst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time.month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0moisst_prepped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_listed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             _, name, variable = _get_virtual_variable(\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_level_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m             )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, level_vars, dim_sizes)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "# Remove dates from annual file that overlap with updates.\n",
    "# This will make it so the current month will overwrite as it gets finalized.\n",
    "\n",
    "# Boolean flag for whether time is before the update_month set in beginning\n",
    "def pre_update_month(month):\n",
    "    return (month != update_month)\n",
    "\n",
    "# Subset time so there isn't overlap on update month\n",
    "oisst_prepped = oisst.sel(time = pre_update_month(oisst['time.month']))\n",
    "oisst_prepped\n",
    "\n",
    "# close oisst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (lat: 720, lon: 1440, time: 366, zlev: 1)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "  * time     (time) datetime64[ns] 2020-01-01 2020-01-02 ... 2020-12-31T12:00:00\n",
       "Dimensions without coordinates: zlev\n",
       "Data variables:\n",
       "    sst      (time, lat, lon, zlev) float32 dask.array&lt;chunksize=(322, 240, 288, 1), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 720, lon: 1440, time: 366, zlev: 1)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 -89.875 -89.625 -89.375 ... 89.375 89.625 89.875\n",
       "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.375 359.625 359.875\n",
       "  * time     (time) datetime64[ns] 2020-01-01 2020-01-02 ... 2020-12-31T12:00:00\n",
       "Dimensions without coordinates: zlev\n",
       "Data variables:\n",
       "    sst      (time, lat, lon, zlev) float32 dask.array<chunksize=(322, 240, 288, 1), meta=np.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append/combine updates to previous months\n",
    "oisst_updated = xr.combine_by_coords(datasets = [oisst_prepped, update_prepped])\n",
    "oisst_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Netcdf Attributes\n",
    "\n",
    "Things like the datetime origin, and the other attributes of the array should match the yearly netcdfs we have so that they can all append correctly without information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conventions': 'CF-1.5',\n",
       " 'title': 'NOAA/NCEI 1/4 Degree Daily Optimum Interpolation Sea Surface Temperature (OISST) Analysis, Version 2.1',\n",
       " 'institution': 'NOAA/National Centers for Environmental Information',\n",
       " 'source': 'NOAA/NCEI https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/',\n",
       " 'References': 'https://www.psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html',\n",
       " 'dataset_title': 'NOAA Daily Optimum Interpolation Sea Surface Temperature',\n",
       " 'version': 'Version 2.1',\n",
       " 'comment': 'Reynolds, et al.(2007) Daily High-Resolution-Blended Analyses for Sea Surface Temperature (available at https://doi.org/10.1175/2007JCLI1824.1). Banzon, et al.(2016) A long-term record of blended satellite and in situ sea-surface temperature for climate monitoring, modeling and environmental studies (available at https://doi.org/10.5194/essd-8-165-2016). Huang et al. (2020) Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version v02r01, submitted.Climatology is based on 1971-2000 OI.v2 SST. Satellite data: Pathfinder AVHRR SST and Navy AVHRR SST. Ice data: NCEP Ice and GSFC Ice. Data less than 15 days old may be subject to revision.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take attribute information from oisst_prepped\n",
    "oisst_updated.attrs = oisst_prepped.attrs\n",
    "oisst_updated.attrs\n",
    "\n",
    "\n",
    "#OR, make a manual addition of what they should be based on the other files, so we aren't transferring\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save out Annual File\n",
    "\n",
    "Now that there is data up through the current date, and with all attributes correctly set, the annual file can be saved out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annual_loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fa4ee7355426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build out destination folder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannual_loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnaming_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"sst.day.mean.{update_yr}.v2.nc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{out_folder}{naming_structure}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annual_loc' is not defined"
     ]
    }
   ],
   "source": [
    "# Build out destination folder:\n",
    "out_folder = annual_loc\n",
    "naming_structure = f\"sst.day.mean.{update_yr}.v2.nc\"\n",
    "out_path = f\"TESTING{out_folder}{naming_structure}\"\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close oisst and oisst_prepped\n",
    "oisst.close()\n",
    "oisst_prepped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable 'sst' has conflicting _FillValue (nan) and missing_value (-9.969209968386869e+36). Cannot encode data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4a38ddf756fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Export the finished file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moisst_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1543\u001b[0m             \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1545\u001b[0;31m             \u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1546\u001b[0m         )\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;31m# to be parallelized with dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         dump_to_store(\n\u001b[0;32m-> 1089\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         )\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, variables, attributes)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# All NetCDF files get CF encoded by default, without this attempting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# to write times, for example, would fail.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcf_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mcf_encoder\u001b[0;34m(variables, attributes)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0m_update_bounds_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0mnew_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_cf_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;31m# Remove attrs from bounds variables (issue #2921)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0m_update_bounds_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0mnew_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_cf_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;31m# Remove attrs from bounds variables (issue #2921)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mencode_cf_variable\u001b[0;34m(var, needs_copy, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsignedIntegerCoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     ]:\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# TODO(shoyer): convert all of these to use coders, too:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/coding/variables.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m         ):\n\u001b[1;32m    160\u001b[0m             raise ValueError(\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;34mf\"Variable {name!r} has conflicting _FillValue ({fv}) and missing_value ({mv}). Cannot encode data.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable 'sst' has conflicting _FillValue (nan) and missing_value (-9.969209968386869e+36). Cannot encode data."
     ]
    }
   ],
   "source": [
    "# Export the finished file\n",
    "#oisst_updated.to_netcdf(path = out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
