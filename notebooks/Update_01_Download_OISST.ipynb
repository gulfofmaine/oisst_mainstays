{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update - Download New OISST Data'\n",
    "\n",
    "This is basically the babystep towards continuous integration. Use Beautifulsoup to ping the url for re-downloading the most recent OISST data. This step will then kick off subsequent `Update`  workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the download information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update year to search for among links\n",
    "update_yr = 2020\n",
    "update_month = 12\n",
    "\n",
    "# Out Destination - Cache gobals\n",
    "_cache_root = \"RES Data/OISST/oisst_mainstays\"\n",
    "\n",
    "# Cache Subdirectory Locations\n",
    "cache_locs = {\n",
    "  \"cache_root\" : _cache_root,\n",
    "  \"annual_file_cache\" : _cache_root + \"/oisst_annual_sst/\" ,\n",
    "  \"regional_cache\" : _cache_root + \"/regional_sst_timelines/\" \n",
    "}\n",
    "\n",
    "# Set output location\n",
    "cache_loc = cache_locs[\"annual_file_cache\"]\n",
    "\n",
    "# Root url where the yearly files are stored\n",
    "\n",
    "# This url does  not work as it is the return for a data query. When using bs4 it scrapes the site prior to the search\n",
    "#fetch_url = \"https://psl.noaa.gov/cgi-bin/db_search/DBListFiles.pl?did=132&tid=89745&vid=2423\"\n",
    "\n",
    "# Use the url from Eric's Repo, thank you Eric\n",
    "\n",
    "fetch_url = f\"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/{update_yr}{update_month}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the http directory listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the http directory listing.\n",
    "req = requests.get(fetch_url)\n",
    "\n",
    "# Print error message if link does not work\n",
    "if req.status_code != requests.codes.ok:\n",
    "    #logger.info(\"{} {} {}\".format(req.status_code, req.reason, req.url))\n",
    "    print(f\"Request Error, Reason: {req.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the destination folder connection. would be weird if it did not exist, but check anyways. Going to keep all the years in one folder so that they are easier to load with xr.open_mfdataset(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse req with BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Parse it with BS and its html parser.\n",
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oisst-avhrr-v02r01.20201201_preliminary.nc\n",
      "oisst-avhrr-v02r01.20201202_preliminary.nc\n",
      "oisst-avhrr-v02r01.20201203_preliminary.nc\n",
      "oisst-avhrr-v02r01.20201204_preliminary.nc\n"
     ]
    }
   ],
   "source": [
    "# Find all href anchors in the html text\n",
    "anchors = soup.find_all(\"a\")\n",
    "for link in anchors:\n",
    "    #print(link)\n",
    "    if link.get(\"href\").endswith(\"nc\"):\n",
    "        href = link.get(\"href\")\n",
    "        print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/202012/oisst-avhrr-v02r01.20201201_preliminary.nc\n",
      "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/202012/oisst-avhrr-v02r01.20201202_preliminary.nc\n",
      "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/202012/oisst-avhrr-v02r01.20201203_preliminary.nc\n",
      "https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/202012/oisst-avhrr-v02r01.20201204_preliminary.nc\n"
     ]
    }
   ],
   "source": [
    "# Find all the links in fetch_url which end with \".nc\"\n",
    "for link in soup.find_all('a'):\n",
    "    \n",
    "    # Find links that match update year\n",
    "    if link.get('href').endswith(f'.nc'):\n",
    "        \n",
    "        # Get the link(s) that match\n",
    "        href = link.get('href')\n",
    "        #print(f\"Download link match: {href}\")\n",
    "        \n",
    "        # Use requests to download\n",
    "        full_file = \"/oisst-avhrr-v02r01.\"\n",
    "        req_link = fetch_url + href\n",
    "        print(req_link)\n",
    "        req = requests.get(fetch_url + href)\n",
    "        if req.raise_for_status():\n",
    "            exit()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####. Status: At this point requests is able to connect with the different files\n",
    "        # It would be desirable to use xarray to append them into a single month instead of daily files\n",
    "        # BUT as single files its possible to keep track of the prelim data\n",
    "        # OR if this is done daily and prelim files just get replaced, then you can just note that and save the month\n",
    "        # The problem there is you need to replace the last two months until that 14 day period is done.\n",
    "        \n",
    "#         # Open link\n",
    "        \n",
    "#         file = open(cache_loc + href, 'wb')\n",
    "#         chunk_size = 17000000\n",
    "        \n",
    "#         # Process in chunks\n",
    "#         for chunk in req.iter_content(chunk_size):\n",
    "#             file.write(chunk)\n",
    "#         file.close()\n",
    "#         print(f\"Created {cache_loc} {href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
